[settings]
batchSize = 64
numEpochs = 30
architecture = 'FFN'

# define network architecture
# possible layer types are 'dense', 'lowRank', and 'vanillaLowRank', 'flatten',
# possible activations are 'relu', 'linear', 'sigmoid', 'tanh'

[[layer]]
type = 'flatten'

[[layer]]
type = 'lowRank'
dims = [784,128]
activation = 'relu'
rank = 20

[[layer]]
type = 'lowRank'
dims = [128, 64]
activation = 'relu'
rank = 20


[[layer]]
type = 'dense'
dims=[64,10]
activation = 'linear'


# ---------- Optimizer Config --------------

[optimizer.default]
type = 'adam'
parameters = { lr = 0.005 }

[optimizer.lowRank]
type = 'DynamicLowRankOptimizer'
parameters = { lr = 0.05 }

[optimizer.vanillaLowRank]
type = 'SimpleSGD'
parameters = { lr = 0.005 }